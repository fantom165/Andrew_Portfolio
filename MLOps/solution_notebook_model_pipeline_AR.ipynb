{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# End-To_End Pipeline - MS Azure ML Studio: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The objective is to analyze the relationship between advertising expenditures across different media sources, including TV, Radio, and Newspaper, and their impact on sales performance. The goal is to determine the optimal allocation of advertising budgets across these channels to maximize sales revenue. By examining the correlation between advertising expenditures and sales, the company aims to identify the most effective advertising strategies and make data-driven decisions to improve its marketing efforts. The insights gained from this analysis will enable the company to optimize resource allocation and develop a robust advertising strategy that aligns with its sales objectives.\n",
        "\n",
        "Furthermore, the business seeks to create a pipeline for automation purposes. This automation pipeline will save time and resources. The business aims to leverage automation to increase efficiency, accuracy, and agility in optimizing resource allocation and developing a robust advertising strategy that aligns with its sales objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**General Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1688841700374
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging # Importing the logging module for logging purposes\n",
        "import json # Importing the json module for working with JSON data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Azure ML imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1688841771837
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#pip install azure.ai.ml\n",
        "import azure.ai.ml\n",
        "from azure.ai.ml import MLClient # Importing the MLClient class from the azure.ai.ml module\n",
        "from azure.identity import DefaultAzureCredential # Importing the DefaultAzureCredential class from the azure.identity module\n",
        "from azure.ai.ml import command, Input, Output, dsl  # Importing the command, Input, Output, and dsl classes from the azure.ai.ml module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Azure authentication**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1688841885310
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1688841902159
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"8c0836c0-f63c-4878-aac9-0566796a0de8\",\n",
        "    resource_group_name=\"test-resource\",\n",
        "    workspace_name=\"demo_azureML\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1688841927447
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready to work with demo_azureML\n"
          ]
        }
      ],
      "source": [
        "# Get the workspace\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace(subscription_id=\"8c0836c0-f63c-4878-aac9-0566796a0de8\",\n",
        "                resource_group=\"test-resource\",\n",
        "                workspace_name=\"demo_azureML\")\n",
        "print('Ready to work with', ws.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1688841994211
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\")\n",
        "# Get the logger instance for the \"azure.core.pipeline.policies.http_logging_policy\" logger.\n",
        "# This logger is used for logging HTTP requests and responses in the Azure SDK pipeline.\n",
        "logger.setLevel(logging.WARNING)\n",
        "# Set the logging level for the logger to WARNING.\n",
        "# This means that only log messages with a severity level of WARNING or higher will be logged.\n",
        "# Log messages with lower severity levels, such as INFO or DEBUG, will be ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1688842012382
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "az_credentials = DefaultAzureCredential(\n",
        "    exclude_interactive_browser_credential=False\n",
        ")\n",
        "# Create an instance of DefaultAzureCredential.\n",
        "# The DefaultAzureCredential class is used for authenticating with Azure services using default Azure Active Directory credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Upload the dataset as Data asset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1688842240501
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EasyVisa\n",
            "car_mpg_jun\n",
            "advertising_data\n"
          ]
        }
      ],
      "source": [
        "for registered_data in ml_client.data.list():\n",
        "    print(registered_data.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Create a compute resource to run the jobs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1688842324329
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You already have a cluster named cpu-cluster-preprocessing, we'll reuse it as is.\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster-preprocessing\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"Standard_D2_v3\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    print(\n",
        "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
        "    )\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Processing Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1688842742849
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# The code retrieves a specific version of a registered data asset using the ml_client object.\n",
        "ad_sales = ml_client.data.get(\"advertising_data\", version=\"1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1688843013843
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating a folder to store data-prep script\n",
        "import os\n",
        "\n",
        "## Set the name of the directory we want to create\n",
        "dependencies_dir = \"./data_prep\"\n",
        "\n",
        "# # The os.makedirs() function creates a directory\n",
        "# exist_ok=True means that the function will not raise an exception if the directory already exists\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing data_prep/tts.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_prep/tts.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.2)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "\n",
        "    df = pd.read_csv(args.data)\n",
        "\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=args.test_train_ratio,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Define data preparation steps as a command**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1688843289471
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define a data preparation step\n",
        "step_process = command(\n",
        "    name=\"data_prep_ad_sales\", # Name of the step\n",
        "    display_name=\"Data preparation for training\", # Display name of the step\n",
        "    description=\"read a .csv input, split the input to train and test\", # Description of the step\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"), # Input named \"data\" of type \"uri_folder\"\n",
        "        \"test_train_ratio\": Input(type=\"number\"), # Input named \"test_train_ratio\" of type \"number\"\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),  # Output named \"train_data\" of type \"uri_folder\" with read-write mount mode\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"), # Output named \"test_data\" of type \"uri_folder\" with read-write mount mode\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code='data_prep/', # Source folder of the component\n",
        "    command=\"\"\"python tts.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            \"\"\", # Command to be executed\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\", # Environment for executing the command\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1688843741817
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CommandComponent({'auto_increment_version': True, 'source': 'BUILDER', 'is_anonymous': False, 'name': 'data_prep_ad_sales', 'description': 'read a .csv input, split the input to train and test', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f0490981c90>, 'command': 'python tts.py             --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}}             --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}}             ', 'code': 'data_prep/', 'environment_variables': None, 'environment': 'AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'Data preparation for training', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_folder'}, 'test_train_ratio': {'type': 'number'}}, 'outputs': {'train_data': {'type': 'uri_folder', 'mode': 'rw_mount'}, 'test_data': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {}})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#The code retrieves the component associated with the step_process\n",
        "step_process.component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 2: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1688843846227
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating a folder to store training scritp script\n",
        "import os\n",
        "\n",
        "## Set the name of the directory we want to create\n",
        "dependencies_dir = \"./train\"\n",
        "\n",
        "# # The os.makedirs() function creates a directory\n",
        "# exist_ok=True means that the function will not raise an exception if the directory already exists\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Training Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train/gbr.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train/gbr.py\n",
        "\n",
        "import os\n",
        "import mlflow\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "mlflow.start_run() # Start a new MLflow run\n",
        "\n",
        "#os.makedirs(\"./outputs\", exist_ok=True) # Create the \"outputs\" directory if it doesn't exist\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args() # Parse the command-line arguments\n",
        "\n",
        "    ad_sales_train = pd.read_csv(select_first_file(args.train_data))  # Read the training data\n",
        "    ad_sales_test = pd.read_csv(select_first_file(args.test_data)) # Read the test data\n",
        "\n",
        "    target = 'Sales'\n",
        "    numeric_features = ['TV','Radio','Newspaper']\n",
        "\n",
        "    # Extract the features from the training data\n",
        "    X_train = ad_sales_train.drop(columns=[target]) \n",
        "    y_train = ad_sales_train[target]\n",
        "\n",
        "    # Extract the features from the test data\n",
        "    X_test = ad_sales_test.drop(columns=[target])\n",
        "    y_test = ad_sales_test[target]\n",
        "\n",
        "    # Create a column transformer for preprocessing the numeric features\n",
        "    preprocessor = make_column_transformer(\n",
        "        (StandardScaler(), numeric_features)\n",
        "    )\n",
        "\n",
        "    # Create a Gradient Boosting Regressor model\n",
        "    model_gbr = GradientBoostingRegressor(\n",
        "        n_estimators=args.n_estimators,\n",
        "        learning_rate=args.learning_rate\n",
        "    )\n",
        "\n",
        "    # Create a pipeline with preprocessing and the model\n",
        "    model_pipeline = make_pipeline(preprocessor, model_gbr)\n",
        "\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    rmse = model_pipeline.score(X_test, y_test)\n",
        "\n",
        "    mlflow.log_metric(\"RMSE\", float(rmse))\n",
        "\n",
        "    print(\"Registering model pipeline\")\n",
        "\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model_pipeline,\n",
        "        registered_model_name=\"gbr-ad-sales-predictor\",\n",
        "        artifact_path=\"gbr-ad-sales-predictor\"\n",
        "    ) # Register the model pipeline in MLflow\n",
        "\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Define model training steps as a command**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1688849565364
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "step_train = command(\n",
        "    name=\"train_ad_sales_model\",\n",
        "    display_name=\"Training an Ad sales model\",\n",
        "    description=\"read a .csv input, split the input to train and test\",\n",
        "    inputs={\n",
        "        \"train_data\": Input(type=\"uri_folder\"),# Input: Path to the training data (URI folder)\n",
        "        \"test_data\": Input(type=\"uri_folder\"), # Input: Path to the test data (URI folder)\n",
        "        \"learning_rate\": Input(type=\"number\"), # Input: Learning rate for the model (number)\n",
        "        \"registered_model_name\": Input(type=\"string\") # Input: Name for the registered model (string)\n",
        "    },\n",
        "    outputs=dict(\n",
        "        model=Output(type=\"uri_folder\", mode=\"rw_mount\") # Output: Path to the trained model (URI folder)\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code='train/',\n",
        "    command=\"\"\"python gbr.py \\\n",
        "              --train_data ${{inputs.train_data}} \\\n",
        "              --test_data ${{inputs.test_data}} \\\n",
        "              --learning_rate ${{inputs.learning_rate}} \\\n",
        "              --registered_model_name ${{inputs.registered_model_name}} \\\n",
        "              --model ${{outputs.model}}\n",
        "            \"\"\",\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",  # Execution environment for the command\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1688849569957
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CommandComponent({'auto_increment_version': True, 'source': 'BUILDER', 'is_anonymous': False, 'name': 'train_ad_sales_model', 'description': 'read a .csv input, split the input to train and test', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f0478d44af0>, 'command': 'python gbr.py               --train_data ${{inputs.train_data}}               --test_data ${{inputs.test_data}}               --learning_rate ${{inputs.learning_rate}}               --registered_model_name ${{inputs.registered_model_name}}               --model ${{outputs.model}}\\n            ', 'code': 'train/', 'environment_variables': None, 'environment': 'AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'Training an Ad sales model', 'is_deterministic': True, 'inputs': {'train_data': {'type': 'uri_folder'}, 'test_data': {'type': 'uri_folder'}, 'learning_rate': {'type': 'number'}, 'registered_model_name': {'type': 'string'}}, 'outputs': {'model': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {}})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "step_train.component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 3: Assemble Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Define the pipeline structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1688849574465
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    compute='cpu-cluster-preprocessing',\n",
        "    description=\"data preparation and training pipeline\"\n",
        ")\n",
        "def ad_sales_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    data_prep_job = step_process(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = step_train(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1688849578727
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "registered_model_name = \"ad_sales_model_v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This function converts each of the steps defined before to be jobs by providing the values for the parameters. For the training job the component attributes are accessed to enable connections between components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Create and Run the Pipeline job**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1688849582168
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipeline = ad_sales_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=ad_sales.path), # Input path\n",
        "    pipeline_job_test_train_ratio=0.25, # Input: Test-train ratio for data splitting\n",
        "    pipeline_job_learning_rate=0.05, # Input: Learning rate for the model\n",
        "    pipeline_job_registered_model_name=registered_model_name,  # Input: Name for the registered model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1688849591975
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading train (0.0 MBs): 100%|██████████| 3141/3141 [00:00<00:00, 71151.71it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"Training pipeline with registered components\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1688850135283
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunId: witty_helmet_vhyy1bdt44\n",
            "Web View: https://ml.azure.com/runs/witty_helmet_vhyy1bdt44?wsid=/subscriptions/8c0836c0-f63c-4878-aac9-0566796a0de8/resourcegroups/test-resource/workspaces/demo_azureML\n",
            "\n",
            "Streaming logs/azureml/executionlogs.txt\n",
            "========================================\n",
            "\n",
            "[2023-07-08 20:53:12Z] Completing processing run id 59dfe145-54b8-4871-aed7-42eaf436d1a1.\n",
            "[2023-07-08 20:53:13Z] Submitting 1 runs, first five are: fde3f4ef:21ea45b0-3632-4cad-9d1a-7c87144e9fc9\n",
            "[2023-07-08 21:01:20Z] Completing processing run id 21ea45b0-3632-4cad-9d1a-7c87144e9fc9.\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: witty_helmet_vhyy1bdt44\n",
            "Web View: https://ml.azure.com/runs/witty_helmet_vhyy1bdt44?wsid=/subscriptions/8c0836c0-f63c-4878-aac9-0566796a0de8/resourcegroups/test-resource/workspaces/demo_azureML\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**----------------------------------------------------------------------------------------------------------------------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
