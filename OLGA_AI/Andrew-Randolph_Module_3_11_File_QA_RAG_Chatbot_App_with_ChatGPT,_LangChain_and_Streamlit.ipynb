{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPRVq3e03c1K"
      },
      "source": [
        "# File QA RAG Chatbot App with ChatGPT, LangChain and Streamlit\n",
        "\n",
        "Here we will implement an advanced RAG System with ChatGPT, LangChain and Streamlit to build a File QA UI-based chatbot with the following features:\n",
        "\n",
        "- PDF Document Upload and Indexing\n",
        "- RAG System for query analysis and response\n",
        "- Result streaming capabilities (Real-time output)\n",
        "- Show document sources of the answer from RAG system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install App and LLM dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "0cb5b8c2-edc9-4a3b-ab74-4b4549b934a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.1.12 in c:\\users\\arand\\anaconda3\\lib\\site-packages (0.1.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (1.4.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (0.0.29)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (0.1.53)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (0.1.142)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (1.24.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain==0.1.12) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (24.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (5.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.12) (3.0.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain==0.1.12) (23.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (3.10.6)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.12) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.12) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.12) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.12) (2022.9.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.12) (1.1.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (1.0.5)\n",
            "Requirement already satisfied: sniffio in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (1.2.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.12) (0.4.3)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\arand\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (1.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.27.4 in c:\\users\\arand\\anaconda3\\lib\\site-packages (0.27.4)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai==0.27.4) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai==0.27.4) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai==0.27.4) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (2022.9.14)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.4) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.4) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.4) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.4) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.4) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.4) (1.3.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\arand\\anaconda3\\lib\\site-packages (from tqdm->openai==0.27.4) (0.4.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain==0.1.12\n",
        "%pip install openai==0.27.4\n",
        "# %pip install langchain-openai==0.0.8\n",
        "# %pip install langchain-community==0.0.29\n",
        "# %pip install streamlit==1.32.2\n",
        "# %pip install PyMuPDF==1.24.0\n",
        "# %pip install chromadb==0.4.24\n",
        "# %pip install pyngrok==7.1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\arand\\anaconda3\\lib\\site-packages (0.27.4)\n",
            "Collecting openai\n",
            "  Downloading openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\arand\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.9.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\arand\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\arand\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.5)\n",
            "Downloading openai-1.66.3-py3-none-any.whl (567 kB)\n",
            "   ---------------------------------------- 567.4/567.4 kB 6.5 MB/s eta 0:00:00\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.27.4\n",
            "    Uninstalling openai-0.27.4:\n",
            "      Successfully uninstalled openai-0.27.4\n",
            "Successfully installed openai-1.66.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwGjVWK4q6F"
      },
      "source": [
        "## Load OpenAI API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5e1HqI56y7t3"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ryheOZuXxa41"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open('openai_key.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZs7ts6NzADJ",
        "outputId": "307734e2-8edf-4d13-f58d-8e363f671d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['openai_key'])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "api_creds.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#(api_creds['openai_key'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMshwB1U9iQ"
      },
      "source": [
        "## Write the app code here and store it in a py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXceMDF0Qza0",
        "outputId": "62a23071-134b-40ae-91ca-6e3c3202f11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting olgafn.py\n"
          ]
        }
      ],
      "source": [
        "#%%writefile olgafn.py\n",
        "# the following line is a magic command\n",
        "# that will write all the code below it to the python file app.py\n",
        "# we will then deploy this app.py file on the cloud server where colab is running\n",
        "# if you have your own server you can just write the code in app.py and deploy it directly\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "import yaml\n",
        "with open('openai_key.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)\n",
        "    \n",
        "api_creds.keys()\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"File QA Chatbot\", page_icon=\"🤖\")\n",
        "st.title(\"Welcome to OLGA QA RAG Chatbot 🤖\")\n",
        "st.image(\"logo.jpg\", width=400) #add logo. Adjust width as necessary\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "# Takes uploaded PDFs, creates document chunks, computes embeddings\n",
        "# Stores document chunks and embeddings in a Vector DB\n",
        "# Returns a retriever which can look up the Vector DB\n",
        "# to return documents based on user input\n",
        "# Stores this in the cache\n",
        "def configure_retriever(uploaded_files):\n",
        "  # Read documents\n",
        "  docs = []\n",
        "  temp_dir = tempfile.TemporaryDirectory()\n",
        "  for file in uploaded_files:\n",
        "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "    with open(temp_filepath, \"wb\") as f:\n",
        "      f.write(file.getvalue())\n",
        "    loader = PyMuPDFLoader(temp_filepath)\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "  # Split into documents chunks\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
        "                                                 chunk_overlap=200)\n",
        "  doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "  # Create document embeddings and store in Vector DB\n",
        "  embeddings_model = OpenAIEmbeddings(openai_api_base=\"https://api.openai.com/v1\")\n",
        "  #embeddings_model = OpenAIEmbeddings()\n",
        "  vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
        "\n",
        "  # Define retriever object\n",
        "  retriever = vectordb.as_retriever()\n",
        "  return retriever\n",
        "\n",
        "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
        "# to an existing text stream and rendering the updated text in Markdown\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "  def __init__(self, container, initial_text=\"\"):\n",
        "    self.container = container\n",
        "    self.text = initial_text\n",
        "\n",
        "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "    self.text += token\n",
        "    self.container.markdown(self.text)\n",
        "\n",
        "# Creates UI element to accept PDF uploads\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "  label=\"Upload PDF files here\", type=[\"pdf\"],\n",
        "  accept_multiple_files=True\n",
        ")\n",
        "if not uploaded_files:\n",
        "  st.info(\"Please upload PDF documents to continue.\")\n",
        "  st.stop()\n",
        "\n",
        "# Create retriever object based on uploaded PDFs\n",
        "retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "# Load a connection to ChatGPT LLM\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0,\n",
        "                     streaming=True)\n",
        "\n",
        "# Create a prompt template for QA RAG System\n",
        "qa_template = \"\"\"\n",
        "              Use only the following pieces of context to answer the question at the end.\n",
        "              If you don't know the answer, just say that you don't know,\n",
        "              don't try to make up an answer. Keep the answer as concise as possible.\n",
        "\n",
        "              {context}\n",
        "\n",
        "              Question: {question}\n",
        "              \"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "# This function formats retrieved documents before sending to LLM\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# Create a QA RAG System Chain\n",
        "qa_rag_chain = (\n",
        "  {\n",
        "    \"context\": itemgetter(\"question\") # based on the user question get context docs\n",
        "      |\n",
        "    retriever\n",
        "      |\n",
        "    format_docs,\n",
        "    \"question\": itemgetter(\"question\") # user question\n",
        "  }\n",
        "    |\n",
        "  qa_prompt # prompt with above user question and context\n",
        "    |\n",
        "  chatgpt # above prompt is sent to the LLM for response\n",
        ")\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "# Shows the first message when app starts\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "  streamlit_msg_history.add_ai_message(\"What is your name and how can I help you?\")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# Callback handler which does some post-processing on the LLM response\n",
        "# Used to post the top 3 document sources used by the LLM in RAG response\n",
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "  def __init__(self, msg: st.write):\n",
        "    BaseCallbackHandler.__init__(self)\n",
        "    self.msg = msg\n",
        "    self.sources = []\n",
        "\n",
        "  def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
        "    source_ids = []\n",
        "    for d in documents: # retrieved documents from retriever based on user query\n",
        "      metadata = {\n",
        "        \"source\": d.metadata[\"source\"],\n",
        "        \"page\": d.metadata[\"page\"],\n",
        "        \"content\": d.page_content[:100]\n",
        "      }\n",
        "      idx = (metadata[\"source\"], metadata[\"page\"])\n",
        "      if idx not in source_ids: # store unique source documents\n",
        "        source_ids.append(idx)\n",
        "        self.sources.append(metadata)\n",
        "\n",
        "  def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "    if len(self.sources):\n",
        "      st.markdown(\"__Sources:__ \"+\"\\n\")\n",
        "      st.dataframe(data=pd.DataFrame(self.sources[:3]),\n",
        "                    width=1000) # Top 3 sources\n",
        "\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    # Initializing an empty data stream\n",
        "    stream_handler = StreamHandler(st.empty())\n",
        "    # UI element to write RAG sources after LLM response\n",
        "    sources_container = st.write(\"\")\n",
        "    pm_handler = PostMessageHandler(sources_container)\n",
        "    config = {\"callbacks\": [stream_handler, pm_handler]}\n",
        "    # Get LLM response\n",
        "    response = qa_rag_chain.invoke({\"question\": user_prompt},\n",
        "                                    config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8de1tM6FVLsq"
      },
      "source": [
        "## Start the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za_TAI2RkPI9"
      },
      "outputs": [],
      "source": [
        "#!streamlit run app.py --server.port=8989 &>./logs.txt &\n",
        "streamlit run olgafn.py server.port=8989 *>./logs.txt 2>&1\n",
        "\n",
        "# run in the background without opening a new window. Grab the url from the .txt file\n",
        "#Start-Process -FilePath \"chainlit\" -ArgumentList \"run olga.py --port=8989 --watch\" -RedirectStandardOutput \"logs.txt\" -RedirectStandardError \"logs.txt\" -NoNewWindow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrVhyQVirAqP",
        "outputId": "eda335b1-37b9-464b-906d-935fb06fe1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streamlit App: https://70cd-35-185-85-25.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# from pyngrok import ngrok\n",
        "# import yaml\n",
        "\n",
        "# # Terminate open tunnels if exist\n",
        "# ngrok.kill()\n",
        "\n",
        "# # Setting the authtoken\n",
        "# # Get your authtoken from `ngrok_credentials.yml` file\n",
        "# with open('ngrok_credentials.yml', 'r') as file:\n",
        "#     NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
        "# ngrok.set_auth_token(NGROK_AUTH_TOKEN['ngrok_key'])\n",
        "\n",
        "# # Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "# ngrok_tunnel = ngrok.connect(8989)\n",
        "# print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q3yFB_jsgC5"
      },
      "source": [
        "## Remove running app processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pG7Abg_LrAw6"
      },
      "outputs": [],
      "source": [
        "#ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2VA4ZtCkPNN",
        "outputId": "ac5397d0-5f30-454b-d97b-45023cf36a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root        1110       1  1 05:29 ?        00:00:20 /usr/bin/python3 /usr/local/bin/streamlit run ap\n",
            "root        9062     306  0 06:02 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        9064    9062  0 06:02 ?        00:00:00 grep streamlit\n"
          ]
        }
      ],
      "source": [
        "!ps -ef | grep streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WxGAxGHkPLP"
      },
      "outputs": [],
      "source": [
        "!sudo kill -9 1110"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "import yaml\n",
        "with open('openai_key.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)\n",
        "    \n",
        "api_creds.keys()\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#(api_creds['openai_key'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This confirms my API key is valid and making calls to this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlBQh5fdkPPY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Access confirmed. Embedding response: client=<openai.resources.embeddings.Embeddings object at 0x000001C170C27880> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001C170CCDFD0> model='text-embedding-ada-002' dimensions=None deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=set() disallowed_special='all' chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={'input': 'Test text'} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "import openai\n",
        "openai.api_key = 'openai_key'\n",
        "\n",
        "try:\n",
        "    response = OpenAIEmbeddings(\n",
        "        input=\"Test text\",\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    print(\"Access confirmed. Embedding response:\", response)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.43.2\n"
          ]
        }
      ],
      "source": [
        "import streamlit\n",
        "print(streamlit.__version__)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
