{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAGGLE COMPETITION  \n",
    "### Predict House Prices  \n",
    "\n",
    "I am predicting house prices using different Opensource Algorithms to see which scores the best\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading in several python packages that will be used. \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import norm, skew\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "house_train= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "house_test= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n",
    "print(\"Dataset shape:\",'house_train', house_train.shape, 'house_test', house_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SalesPrice correlation with all the feature\n",
    "plt.figure(figsize=(8, 12))\n",
    "house_train.corr()['SalePrice'].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap for top 10 Sales Price-features correlation\n",
    "k = 10\n",
    "cols = house_train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "k_corr_matrix = house_train[cols].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(k_corr_matrix, annot=True, cmap=plt.cm.RdBu_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Correlation** assumes data should be related linearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot to verify linear relationship\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(house_train[cols], size = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is Target variable Normal?\n",
    "target = house_train['SalePrice']\n",
    "f, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sns.distplot(target, kde=False, fit=stats.johnsonsu, ax=axes[0])\n",
    "sns.distplot(target, kde=False, fit=stats.norm, ax=axes[1])\n",
    "sns.distplot(target, kde=False, fit=stats.lognorm, ax=axes[ 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that SalePrice ***doesn't follow normal distribution***, so before performing regression it has to be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying log transformation\n",
    "house_train['SalePrice'] = np.log1p(house_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution histogram and normal probability plot\n",
    "(mu, sigma) = norm.fit(house_train['SalePrice'])\n",
    "sns.distplot(house_train['SalePrice'], fit=norm)\n",
    "plt.legend(['Normal dist ($\\mu=${:.2f}, $\\sigma=${:.2f})'.format(mu, sigma)])\n",
    "\n",
    "fig = plt.figure()\n",
    "stats.probplot(house_train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Outliers in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(x, y, top=5, plot=True):\n",
    "    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n",
    "    x_ =np.array(x).reshape(-1,1)\n",
    "    preds = lof.fit_predict(x_)\n",
    "    lof_scr = lof.negative_outlier_factor_\n",
    "    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n",
    "    if plot:\n",
    "        f, ax = plt.subplots(figsize=(9, 6))\n",
    "        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n",
    "    return out_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data and see if there're any outlier points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GrLivArea-SalePrice outlier detection\n",
    "outs = detect_outliers(house_train['GrLivArea'], house_train['SalePrice'],top=5) \n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating qualitative(categorical) and quantitative(continuous) featues\n",
    "quantitative = [feature for feature in house_train.columns if house_train.dtypes[feature] != 'object']\n",
    "quantitative.remove('SalePrice')\n",
    "quantitative.remove('Id')\n",
    "qualitative = [feature for feature in house_train.columns if house_train.dtypes[feature] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers for all quantitative features\n",
    "from collections import Counter\n",
    "all_outliers=[]\n",
    "\n",
    "for feature in quantitative:\n",
    "    try:\n",
    "        outs = detect_outliers(house_train[feature], house_train['SalePrice'],top=5, plot=False)\n",
    "    except:\n",
    "        continue\n",
    "    all_outliers.extend(outs)\n",
    "\n",
    "print(Counter(all_outliers).most_common())\n",
    "\n",
    "outliers = [30, 88, 462, 523, 632, 1298, 1324] #\n",
    "for i in outliers:\n",
    "    if i in all_outliers:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete outliers from training dataset\n",
    "house_train = house_train.drop(house_train.index[outliers])\n",
    "house_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train.reset_index(drop=True, inplace=True)\n",
    "y_train = house_train['SalePrice']\n",
    "X_train = house_train.drop(['SalePrice'], axis=1)\n",
    "X_test = house_test\n",
    "\n",
    "print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NA's of the quantitative features  \n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerics_train = []; numerics_test = []\n",
    "for i in house_train.columns: \n",
    "    if house_train[i].dtype in numeric_dtypes:\n",
    "        numerics_train.append(i)\n",
    "house_train.update(house_train[numerics_train].fillna(0)) #Filling NA's of training dataset\n",
    "\n",
    "for i in house_test.columns:\n",
    "    if house_test[i].dtype in numeric_dtypes:\n",
    "        numerics_test.append(i)\n",
    "house_test.update(house_test[numerics_test].fillna(0)) #Filling NA's of test dataset\n",
    "#house_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness check and correction using boxcop for quantitative/continuous features\n",
    "skew_train = house_train[quantitative].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "high_skew_train = skew_train[skew_train > 0.5] #skewness value\n",
    "for i in high_skew_train.index:\n",
    "    house_train[i] = boxcox1p(house_train[i], boxcox_normmax(house_train[i] + 1))\n",
    "\n",
    "skew_test = house_test[quantitative].apply(lambda x: skew(x)).sort_values(ascending=False)    \n",
    "high_skew_test = skew_test[skew_train > 0.5]\n",
    "for i in high_skew_test.index:\n",
    "    house_test[i] = boxcox1p(house_test[i], boxcox_normmax(house_test[i] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard scaling to verify boxcox transformation\n",
    "sale_price_scaled = StandardScaler().fit_transform(house_train['SalePrice'][:, np.newaxis])\n",
    "\n",
    "sns.distplot(sale_price_scaled, fit=norm)\n",
    "\n",
    "low_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[:5]]\n",
    "high_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[-5:]]\n",
    "print(f'outer range (low) of the distribution: \\n{low_range}')\n",
    "print(f'outer range (high) of the distribution: \\n{high_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining train and test datasets\n",
    "all_data = pd.concat([X_train, house_test], axis=0, sort=False)\n",
    "all_data.drop(['Id'], axis=1, inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the non-numeric predictors are stored as numbers; we convert them into strings \n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating missing data\n",
    "na_total = all_data.isnull().sum().sort_values(ascending=False)\n",
    "na_ratio = (all_data.isnull().sum() / all_data.shape[0]).sort_values(ascending=False)\n",
    "missing_data = pd.concat([na_total, na_ratio], axis=1, keys=['Total', 'Ratio'])\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most value of these 4 features are missing and they have no pattern , just delete them\n",
    "all_data.drop(['PoolQC', 'Utilities', 'Street', 'MiscFeature', ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling NA with None for categorical features\n",
    "for col in ('Alley','Fence','FireplaceQu','GarageQual','GarageFinish','GarageCond','GarageType','BsmtExposure',\n",
    "          'BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1'):\n",
    "     all_data[col] = all_data[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[all_data['GarageCars'].isnull()][['GarageArea', 'GarageCars', 'GarageType', 'GarageYrBlt', 'GarageQual']])\n",
    "all_data['GarageArea'].fillna(0, inplace=True)\n",
    "all_data['GarageCars'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[all_data['TotalBsmtSF'].isnull()][\n",
    "    ['TotalBsmtSF', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath']])\n",
    "for col in ('TotalBsmtSF', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath'):\n",
    "     all_data[col] = all_data[col].fillna(0)\n",
    "# all_data['TotalBsmtSF'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MasVnrType'].fillna('None', inplace=True)\n",
    "all_data['HasMasVnr'] = all_data['MasVnrType'].apply(lambda x: 0 if x == 'None' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_data.iloc[:len(y_train), :]\n",
    "X_test = all_data.iloc[len(y_train):, :]\n",
    "print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the NA with the mode, which means most categorical type of the feature-train &test\n",
    "X_train['MSZoning'] = X_train.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "X_test['MSZoning'] = X_test.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "for col in ('Functional','Exterior1st','Electrical','KitchenQual','SaleType','Exterior2nd'):\n",
    "    X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n",
    "    X_test[col] = X_test[col].fillna(X_test[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['LotFrontage'] = X_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "X_train['GarageYrBlt'] = (X_train['YearBuilt'] + X_train['YearRemodAdd']) /2\n",
    "X_train['MasVnrArea'] = X_train.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "X_test['LotFrontage'] = X_test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "X_test['GarageYrBlt'] = (X_test['YearBuilt'] + X_test['YearRemodAdd']) /2\n",
    "X_test['MasVnrArea'] = X_test.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['YrBltAndRemod']=X_train['YearBuilt']+X_train['YearRemodAdd']\n",
    "X_train['TotalSF']=X_train['TotalBsmtSF'] + X_train['1stFlrSF'] + X_train['2ndFlrSF']\n",
    "X_train['TotalSqrFootage'] = (X_train['BsmtFinSF1'] + X_train['BsmtFinSF2'] +\n",
    "                                 X_train['1stFlrSF'] + X_train['2ndFlrSF'])\n",
    "X_train['TotalBathrooms'] = (X_train['FullBath'] + (0.5 * X_train['HalfBath']) +\n",
    "                               X_train['BsmtFullBath'] + (0.5 * X_train['BsmtHalfBath']))\n",
    "X_train['TotalPorchSF'] = (X_train['OpenPorchSF'] + X_train['3SsnPorch'] +\n",
    "                              X_train['EnclosedPorch'] + X_train['ScreenPorch'] +\n",
    "                              X_train['WoodDeckSF'])\n",
    "\n",
    "X_test['YrBltAndRemod']=X_test['YearBuilt']+X_test['YearRemodAdd']\n",
    "X_test['TotalSF']=X_test['TotalBsmtSF'] + X_test['1stFlrSF'] + X_test['2ndFlrSF']\n",
    "X_test['TotalSqrFootage'] = (X_test['BsmtFinSF1'] + X_test['BsmtFinSF2'] +\n",
    "                                 X_test['1stFlrSF'] + X_test['2ndFlrSF'])\n",
    "X_test['TotalBathrooms'] = (X_test['FullBath'] + (0.5 * X_test['HalfBath']) +\n",
    "                               X_test['BsmtFullBath'] + (0.5 * X_test['BsmtHalfBath']))\n",
    "X_test['TotalPorchSF'] = (X_test['OpenPorchSF'] + X_test['3SsnPorch'] +\n",
    "                              X_test['EnclosedPorch'] + X_test['ScreenPorch'] +\n",
    "                              X_test['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['has2ndfloor'] = X_train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_train['hasgarage'] = X_train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_train['hasbsmt'] = X_train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_train['hasfireplace'] = X_train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "X_test['has2ndfloor'] = X_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_test['hasgarage'] = X_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_test['hasbsmt'] = X_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_test['hasfireplace'] = X_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "ohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\n",
    "encoded_data=ohe.fit_transform(pd.concat([X_train,X_test], axis=0, sort=False)).reset_index(drop=True)\n",
    "X_train =  encoded_data.iloc[:len(y_train), :]\n",
    "X_test = encoded_data.iloc[len(y_train):, :]\n",
    "print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes colums where the threshold of zero's is (> 99.95), means has only zero values \n",
    "overfit = []\n",
    "len_X_train =len(X_train)\n",
    "\n",
    "for i in X_train.columns:\n",
    "    counts = X_train[i].value_counts()\n",
    "    zeros = counts.iloc[0]\n",
    "    if zeros / len_X_train * 100 > 99.94 :\n",
    "        overfit.append(i)\n",
    "\n",
    "overfit = list(overfit)\n",
    "overfit.append('MSZoning_C (all)')\n",
    "\n",
    "#Converting numpy array to handle XGB feature mismatch error -https://github.com/dmlc/xgboost/issues/2334\n",
    "X_train = np.array(X_train.drop(overfit, axis=1).copy())\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test.drop(overfit, axis=1).copy())\n",
    "\n",
    "print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import opensource algorithms to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_val_score to get the root mean square error, which is the score method for current regression problem\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mse(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, X_train=X_train):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters(for grid search)\n",
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge\n",
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "\n",
    "#lasso\n",
    "lasso = make_pipeline(\n",
    "    RobustScaler(),\n",
    "    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n",
    "\n",
    "#elastic net\n",
    "elasticnet = make_pipeline(\n",
    "    RobustScaler(),\n",
    "    ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n",
    "\n",
    "#svm\n",
    "svr = make_pipeline(RobustScaler(), SVR(\n",
    "    C=20,\n",
    "    epsilon=0.009,\n",
    "    gamma=0.0003,\n",
    "))\n",
    "\n",
    "#GradientBoosting\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000,\n",
    "                                learning_rate=0.05,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                random_state=42)\n",
    "\n",
    "#lightgbm\n",
    "lightgbm = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    num_leaves=4,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000,\n",
    "    max_bin=200,\n",
    "    bagging_fraction=0.75,\n",
    "    bagging_freq=5,\n",
    "    bagging_seed=7,\n",
    "    feature_fraction=0.2,\n",
    "    feature_fraction_seed=7,\n",
    "    verbose=-1,\n",
    "    #min_data_in_leaf=2,\n",
    "    #min_sum_hessian_in_leaf=11\n",
    ")\n",
    "\n",
    "#xgboost reg:squarederror replacing reg:linear\n",
    "xgboost = XGBRegressor(learning_rate=0.01,\n",
    "                       n_estimators=3460,\n",
    "                       max_depth=5,\n",
    "                       min_child_weight=0,\n",
    "                       gamma=0,\n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='reg:squarederror',\n",
    "                       nthread=-1,\n",
    "                       scale_pos_weight=1,\n",
    "                       seed=27,\n",
    "                       reg_alpha=0.00006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StackingCVRegressorï¼šA 'Stacking Cross-Validation' regressor for scikit-learn estimators.\n",
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST score on CV')\n",
    "\n",
    "score = cv_rmse(ridge) #cross_val_score(RidgeCV(alphas),X, y)\n",
    "print(\"Ridge score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_rmse(lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_rmse(elasticnet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_rmse(svr)\n",
    "print(\"SVR score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"Lightgbm score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_rmse(gbr)\n",
    "print(\"GradientBoosting score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_rmse(xgboost)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the stacking model\n",
    "#1.1 learn first-level model\n",
    "#1.2 construct a training set for second-level model\n",
    "#2. train the second-level model\n",
    "#3. re-learn first-level model on the entire train set\n",
    "print('Training Model')\n",
    "stack_gen_model = stack_gen.fit(X_train, y_train) #Fit ensemble regressors and the meta-regressor\n",
    "print('Model Trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit prediction result\n",
    "print('Predict submission')\n",
    "result = np.floor(np.expm1(stack_gen_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame()\n",
    "submission['Id'] = house_test['Id']\n",
    "submission['SalePrice']= result\n",
    "submission.head()\n",
    "submission.to_csv(\"houseprice_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
